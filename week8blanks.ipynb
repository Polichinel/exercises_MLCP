{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4B.0\n",
    "\n",
    "This week we will examine which of our features are most \"important\" for our models ability to predict. This can be relevant if we want to thin-out in our roster of features, thus creating a more parsimonious (i.e. faster) model. It could also aid us regarding where we should concentrated future feature engeneering efforts. Futhermore, it might also hint at what more advanced models we would like to utilize in the future. Lastly there is the controversial question regarding whether such importance measures, as presented below, can tells os anything theoritically about our subject. I'll return to this question in the end of this assigment.   \n",
    "\n",
    "But before we get ahead of ourself, make sure you have executed the week75.ipynb notebook. If not, go do that before continuing here. If you have executed week75, and thus generated and saved the files `train_set_featureEng.pkl` and `val_set_featureEng.pkl`, then you are good to go.\n",
    "\n",
    "Start by importing numpy as np, pandas as pd, geopandas as gpd, matplotlib.pyplot as plt, pickle and from sklearn import metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load your two new pickles `train_set_featureEng.pkl` and `val_set_featureEng.pkl`. If you do not have them, go back and run the week75.ipynb notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_file = open(..., 'rb')\n",
    "train_setFE = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "\n",
    "pkl_file = open(..., 'rb')\n",
    "val_setFE = pickle.load(pkl_file)\n",
    "pkl_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do whatever test(s) make you feel comfortable around your data `train_setFE` and `val_setFE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4B.1 Train a Random Forest Classifier\n",
    "\n",
    "Now, Throughout this exercise we will focus on evaluating features given predictions from a tree-based model. Specifically, we will here use the random forest model. You already worked with this algorithm in 3A and 3B but since it have not yet been on the curriculum I understand if you are not yet overly familiar with Random Forest. Thus, I recommend watching at least the first couple of minutes of this torturial: https://www.youtube.com/watch?v=J4Wdy0Wc_xQ. However, ignore the fact that he use accuracy to evaluate his models since we have greatly imbalanced data and accuracy is thus not appropiate in our case. Also, check out the scikit learn documentation on the random forest classifier https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html?highlight=random%20forest#sklearn.ensemble.RandomForestClassifier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets create our y and our X. Our y should here be `binary_best_t1`. Given that we have so many features now, which we do want to use, I defined X through what we **don't** want to include.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_setFE[...]\n",
    "X_train = train_setFE.drop(['gid','gwno','year','geometry', 'binary_best_t1', 'binary_best_t2', 'binary_best_t3', 'binary_best_t4', 'binary_best_t5'], axis =1)\n",
    "\n",
    "y_val = val_setFE[...]\n",
    "X_val = val_setFE.drop(['gid','gwno','year','geometry', 'binary_best_t1', 'binary_best_t2', 'binary_best_t3', 'binary_best_t4', 'binary_best_t5'], axis =1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, form from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go check your exercise 3A, 3B and and the scikit learn documentation and impliment random forest classifier. Set you parameters. Set random_state = 42. Then fit it to X_train and y_train.\n",
    "\n",
    "You can run it with the same settings as you did in 3A but I recommend you try experimenting a bit with the parameters (but only a bit, we'll formalize the prcedure at a later point). For instance try changing `criterion` from `'gini'` to `'entropy'`. And since we know random forest overfitted last time, lets set max_depth to somewhere between 2 and 6 (Giving the model less *less capacity*). Having a low max_depth might also help visualization futher down.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why would setting max_depth help against overfitting?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = ...\n",
    "rf.fit(..., ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check the results (AUC, AP and BS) for both in- and out of sample prediction as you did in e.g. 3A.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you set the parameters right and implimented your feature engeneering correctly you should do quite well - especially cosindering we a \"only\" using one randome forest model here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4B.2 Feature Importance\n",
    "We will now asses wich features our model found most \"important\". And we will do so by following this blog: https://mljar.com/blog/feature-importance-in-random-forest/. That is, I will supply only very little code below. The rest you should be able to deduce from the blog. Naturally, the author uses different data, but only very few details needs to be changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, try to print the feature importance score (feature_importances_) from our fitted model (rf):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets plot it. And lets skip straight to the plot with sorted (.argsort()) features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if this changes after normalization..\n",
    "\n",
    "plt.figure(figsize = [15,15])\n",
    "\n",
    "sorted_idx = ...\n",
    "plt.barh(X_train.columns[sorted_idx], ...)\n",
    "plt.xlabel(\"Random Forest Feature Importance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4B.3 Permutation importance\n",
    "\n",
    "The blog is a bit quick here, så to better understand the differnece between what we just did above and permutation importance see https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-py. And for more generael guidience, see https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html?highlight=permutation_importance#sklearn.inspection.permutation_importance\n",
    "\n",
    "Now, from sklearn.inspection import permutation_importance, and then follow the blogs guide to Permutation importance. Remember to use X_val and y_val instead of X_test and y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = [15,15])\n",
    "\n",
    "sorted_idx = ...\n",
    "plt.barh(X_train.columns[sorted_idx], ...)\n",
    "plt.xlabel(\"Permutation Importance\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4B.4 SHAP\n",
    "\n",
    "Agian, the author of the blog is a bit fast here, so visit https://github.com/slundberg/shap#citations for more info. In any case you need to install *shap* first. As you know this is done in the termianl and remeMber to do it in the geo_env. The link provides instructions but `conda install -c conda-forge shap` should do it.\n",
    "\n",
    "When *shap* is installed import shap (You might need to restart you notebook to make it work. If so note that there is another package mentioned below, which you might as well install while your at it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now follow the blog to crate the explainer, shap_values and shap importance summary plot. Remeber to use X_val and not X_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = ...\n",
    "shap_values = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values[0], ..., feature_names = X_val.columns, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that for all three importance measure that you've implimented above, the top 13 features (approximately, dependendt of your exact implementation) are related to the temporal pattern of conflict. Most of the conventional, perhaps more theoritical meaty, strutural features just doesn't bring a lot of prediction power to the table after we have included the temporal patterns of conflict. This does not mean that they are not relevant in as potential causes - remember that is not our focus here. It just means that when we have included the temporal pattern of conflict, knowing specificities regard oil, inequlity or ethnic exclusion does not supply much more relevant information. \n",
    "\n",
    "In conventional political science this situation might be attributed to autocorrelation and post-treatment bias. But it still show us that, if we where to go back and do more feature engeneering we might do well if we focused on features regarding the temporal patterns of conflict. Presented in a more theoritical light, the phenomenon of the \"conflict trap\" does seem very impotant for predictive porpuses. Wheter we can then say that the conflcit trap is thus the most important factor for conflict is another matter altogether and I would definitely not be so bold (indeed, the conflcit trap is in it self a product of a lot of other features). But I would say it qualifiess the concept of \"conflict traps\" as something we perhaps should spend a bit more effort studying. For those of you interested `Hegre, H., Nygård, H. M., & Ræder, R. F. (2017). Evaluating the scope and intensity of the conflicttrap:  A dynamic simulation approach. Journal of Peace Research,54(2), 243–261` is a good place to start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An element missing almost entirely from our features pertains to the spatial patter of conflcit. In the litterature called \"conflict diffusion\". After the temporal patterns of conflict the spacial patterns might constitute the most informative potential - so why have we not created any such features (disregarding the crude country wide measures)? Indeed geopandas is able to generate at lot of interesting features here such as distance to other conflicts etc. The reason we did not create such features, doing our feature egeneering effort, is that it takes a very long time. And if we want to take advantage of multiporcessing we have to implement it ourself (last I checked, at least). This is not that hard, but it is a bit beside the point in this course so I leave for the curious to explore another time. \n",
    "\n",
    "What is perhaps even more interesting than raw distance measures is estimation of spatial conflcit patterns. This might be done using Gaussina Processes and Neural networks. these are - as said before - topics too advnced to explore in practice doing this course, but I will say a bit about it in a later lecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  4B.5 Visualize a Decision Tree \n",
    "\n",
    "We will keep working with random forest here, but leave one torturial for another. As you might know by now, random forest consists of a large number of decision trees. Specifically the parameter `n_estimators` denotes the number of decision trees in your model. And conveniently we can plot these individual trees to assess the decisions made. I've stolen and adapted some code from https://towardsdatascience.com/how-to-visualize-a-decision-tree-from-a-random-forest-in-python-using-scikit-learn-38ad2d75f21c below. \n",
    "\n",
    "Before you can run it. however, you might need to install **Graphviz** this can been done like this https://anaconda.org/anaconda/graphviz or like this https://github.com/xflr6/graphviz. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When Graphviz is install succefully in your geo_env, then from sklearn.tree import export_graphviz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, just run the code below. However, try it a couple of times where you look at a new estimator (rf.estimators_[...]) each time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export as dot file\n",
    "\n",
    "estimator = rf.estimators_[99] # if you have n_estimators = 100, then you try from 0 to 99 here\n",
    "\n",
    "export_graphviz(estimator, out_file='tree.dot', \n",
    "                feature_names = X_train.columns,\n",
    "                rounded = True, proportion = False, \n",
    "                precision = 2, filled = True)\n",
    "\n",
    "# Convert to png using system command (requires Graphviz)\n",
    "from subprocess import call\n",
    "call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=60'])\n",
    "\n",
    "# Display in jupyter notebook - if you have high max_depth it might be easier to asses the png directly from to folder so you can zoom in.\n",
    "from IPython.display import Image\n",
    "Image(filename = 'tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So, What does your tree plot show (in more or less generel terms)?**  \n",
    "**Does the connection between max_depth and overfitting appear clear now?**  \n",
    "**What is red and what is blue?**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4B.6 Subset\n",
    "\n",
    "Now, try here to recreate what you did above but now only with a smaller subset of features. Say the top 13 most important features (from any of the three importance measures above) and see waht happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, you might get a bit lower prediction power here, but it should have bin much faster. More importantly, however, notice that all (or most, depending on your implementation) features used here are from the UCDP! that is, they are derived directly from the conflict data. Encouragingly this is the data that is updated way more often than the strutural data from PRIO (Remember?). So, even in this artificial scenario where we actually use prior data as if it was  fully updated, we still manage almost as well with a smaller roster of features exclusivly from UCDP. This is encouraging news given our ambitions of creating reliable and useful predictions in a real-world-scenario. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we are done for now. We will not drop any features yet. since we still whant our whole roster when we go through. Naturally, there are many other ways of assesing features - also when using not-treebased models. But since the last model we are gonna introduce in this course - XGboost - is also tree based, I found it most prudent that we covered this topic. You can look here for other toys to play with: https://scikit-learn.org/stable/modules/feature_selection.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4B.7 (Optional)\n",
    "\n",
    "Repeat the most important steps above but now with one or more new y further out in the \"future\". I.e.:  `binary_best_t2`, `binary_best_t3` ... `binary_best_t5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
