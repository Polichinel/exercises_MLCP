{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exam Part 2 - Return of the Models\n",
    "\n",
    "Before the actual Exam assignment this small introduction is meant to give some initial guidance which will hopefully serve you in time of need. As such, return here when in doubt. Naturally you are also welcome to write me or your peers on discord - though there are some limits to what I will answer. Regarding your peers on discord: As always almost everything is allowed so feel free to share ideas, explanations, elaborations, interpretations, links, videos, tutorials, and so on, **but please do refrain from sharing you own code solutions directly**. Naturally I can't - and don't want to - stop you from sharing code solutions in general, but please take the time to understand why your code did not work while your peer's did work. This also holds true if you find inspiration in the exercises which you provided feedback for throughout the course. Use them aplenty but please do reflect upon what is done. Lastly, it goes without saying that copy-pasting from your own past assignments is 100% legit (even expected).\n",
    "\n",
    "Moving on, the first subsection below will outline what is expected of you in this exam part 2. The second subsection is meant to give you some inspiration should you wish to aim for a high-end-grade. The last subsection before the actual exam gives some advise regarding what to do if one or more exercises proves insurmountable.\n",
    "\n",
    "\n",
    "### Expectations and topics to be covered:\n",
    "\n",
    "In this exam (part 2) we will turn to the actual estimation efforts. That is, using the data you constructed in exam\\_part1 you will now create and asses a number of models and ensembles to predict the future probability of conflict. A key part of any prediction effort is, of course, valid and honest evaluation of our models performance and the uncertainty surrounding our results.\n",
    "\n",
    "As promised, all topics and related code needed to pass this exam have been covered in the weekly exercises. However, you will find that I am a bit less explicit regarding what I want you to do here compared to exam part 1. The end goal of each sub-assignment should still be quite clear but exactly how you implement it is up to you (If you find it unclear, write how you interpret it and follow through). As such this second part of the exam should invite to more creativity on your part compared to part 1. This also goes for plotting. I might ask for a plot but you decide how to plot it. In the later part of the exam knowing what should be plotted or reported is itself part of the challenge. For instance I might ask you to report and/or plot \"relevant results\". Here, determining what is \"relevant results\" and how it should be reported/plotted is up to you. If you have completed the weekly exercises, looked at the other notebooks uploaded such as week12split.ipynb and Some\\_plotsV2.ipynb and read the articles on the curriculum you should have no trouble making such assessments.\n",
    "\n",
    "I have aimed to not make this an \"guess what Simon is thinking\" kind of exam. In Essence the exam is just about making good predictions and evaluations by showing a number of ways we might make models fit our data, showing how we might combat overfitting, showing how we might asses and even quantify some kinds of uncertainty and lastly how we might honestly evaluate and convey our results. If you, when concluding this assignment, think that your achieved this and your code is running (somewhat) smoothly, then it is likely that you will receive a passing, hopefully even satisfying, grade. \n",
    "\n",
    "Of course, I will expect you to explain and comment slightly more on your work than we usually do in the weekly assignments. This is to facilitate some reflection on your part regarding your implementations and decisions. I will not always provide explicit **Questions** but expect you to take some initiative yourselves. In this regard, the exam differs from the weekly exercises. However, while explanations, reflections, augmentations and justifications have not been required up until now, you should be more than qualified to face such challenges given your general academic credentials. References are nice to have, not need to have - your own words and thoughts will do just fine. If you do choose to include references, a bibliography in whatever style you prefer at the end of the exam will be appreciated (but not required).\n",
    "\n",
    "As in part 1, I have strived to let the exam mimic a conventional data science/machine learning workflow. However, especially two considerations makes it a bit of an artificial task. First, we will go through a couple of assignments where models get incrementally harder to implement. While you might also do this IRL, the main purpose is to asses extent of your skills. Secondly, normally we would divide this script in to many smaller scripts - at least one creating and saving predictions and one for making the final plots and evaluations. Here, everything is done in one notebook. So make sure to make \"save-points\" along the way where you pickle any output and can load them in again if need be.\n",
    "\n",
    "As in the first exam, I also expect your notebooks to be somewhat presentable. So while you definitely should try out a lot of different approaches, models and plots, only present your \"crème de la crème\". Or at least try to make the notebook look like something you would not be ashamed to make public on github.\n",
    "\n",
    "### The perfect assignment:\n",
    "Does still not exist. But if you have covered the topics above and you follow the recommendations below your chances of getting a good grade will be very high. My first recommendation follows from the section above: Good written explanations and explicit reflections goes a long way. I am not expecting a novel, just short, on-point, text pieces explaining the jist of what you do and, when relevant, a reflection or two to boot. What naturally makes such efforts even better are references to the curriculum, especially the articles. References beyond the curriculum are also appreciated. This can be in regards to method an subjects alike and it does not have to be a lot. Just showing that you can connect your knowledge to the literature is enough. Also, when applicable explicit math-statements make operations less ambiguous which is appreciated - note that I will reward honest efforts even if the math is slightly off.\n",
    "\n",
    "Another thing which will be greatly appreciated is good \"code hygiene\". That is slick and concise code with little to no redundant python objects haunting your code. The easiest way to achieve this is to put your operations into loops, functions or both when applicable. Even if you do not do it everywhere, your efforts will surely be noticed. As will well-commented code. Again, not to much, but being able to read in the code (#via some comment) what different pieces do is greatly appreciated. And if you do create functions, consider creating a description in the beginning (\"\"\"Such as some function description\"\"\").  \n",
    "\n",
    "Lastly; beautiful plots. This is your chance to score some pretty cheap points. I do appreciate nice plots a lot and python plots are pretty easy to beautify. Note that I say beautify not *pimp*. The reason for this choice of words is that less is often more when it comes to data visualisations. Background colours should be avoided if they serve no purpose. Grids should only be used when necessary, Readable and meaningful scales on y and x axis are mandatory. Indeed if any text appear in or around a given plot this text should be readable. If text is not clearly readable it is redundant almost by definition. The go-to-guy on data visualisation is Edward Tufte. Here is a blog (not by himself though) with some of his points https://moz.com/blog/data-visualization-principles-lessons-from-tufte. As a last note on plots; unnecessary 3D plots, Word Clouds and Pie charts are *not* appreciated.\n",
    "\n",
    "### When it does not work:\n",
    "Shit happens - such is life. If you find yourself close to deadline and unable to implement some code try to show/tell me what you would have done. Make comments in your code telling what each part do, where you think the problem is and what you would have liked the code to do. Also, make comments outside the code elaborating on the issue(s). Why do you think the problem persist? What have you tried to do about it? And just as importantly; what is it that you are trying to do with the code in the first place? If you do know what you are trying to do, please write so, and write what the expected output should have been. Offer me explanations. It is just as important (if not more in practice) that you know what you want to implement as it is that you know how to implement it (youtube will teach you the latter in due time). Also, there's no shame in reaching out to either me or your peers. I am not keeping any scores regarding who asks questions on discord - at least not for grading purpose or to hold it against you.\n",
    "\n",
    "Lastly, I have uploaded pickles corresponding to what you should have generated through exam part 1. You are allowed the use these for this exam part 2 if you get stuck or your own data acts up. But remember to note it if you do so, including an explanation as to why and where (in order to avoid too much confusion on my part). Naturally, if you just use the data for sanity checks doing the coding process, e.g. to see whether this data produce the same results as you own, you don't need to note anything.\n",
    "\n",
    "That's all - may the force be with you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 \n",
    "Import all libraries you will need (you can come back and add to this later) and load the `y_train_set.pkl`, `X_train_set.pkl`, `y_val_set.pkl` and `X_val_set.pkl` created through exam part 1. If you did not get all the way to the end of exam part 1 or are not confident in the data produced, feel free to make use ofthe downloaded the data from Absalon. However, please note in this notebook if you do so.\n",
    "\n",
    "Comment on why we have a training set and a validation set and make a few sanity checks to ensure everything is fine. We only use these sets for now - at the end of this exam we will utilize test sets and train sets for test time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2\n",
    "Report the follow metrics for the two baseline \"models\" (Persistence model and Ratio model): Accuracy, Recall, Precision and the Confusion Matrix. For the ratio model a threshold of 0.5 will do fine here. Do it for both train and validation set. Also do it for all y's (t1, t2, t3, t4, t3). You can opt to simply report the metrics but nice plotting is always appreciated when applicable. Lastly, Argue strength and weaknesses for each metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3\n",
    "\n",
    "Report/plot the following metrics for the ratio baseline model: Brier Score, ROC curve, AUC Score, Precision-Recall Curve and Average Precision score. Preferable, the AUC score and AP score should be present in the ROC and PR-plot respectively. Do it for both train and validation set and for all y's (t1, t2, t3 ,t4 ,t5). Preferable the \"random\" baseline should also be present in the plots. Argue strength and weaknesses for each metric. (While we can do it, we will refrain from evaluating the persistence baseline model via these metrics. Feel free to comment on why these metrics might be imprudent to use with the persistence model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4\n",
    "Explain/comment on why we use baseline models at all?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5\n",
    "Now, train a single model on the train data and test it on the validation data. Do this for all y's (t1, t2, t3, t4, t5). Choose yourself between K-Nearest Neighbors, Naive Bayes, Adaboost and Random forest (so no Xgboost or Ensembles just yet.) Default hyper parameters are fine or you can tweak them a bit if need be, but no grid search just yet. Evaluate the results and report/plot the Brier Score, ROC curve, AUC Score, PR Curve and AP score. When evaluating, remember clear comparisons to baseline models - most importantly the ratio model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.6\n",
    "Present and explain your chosen model and justify why you chose it (intuition is ok - so math is nice to have not need to have)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.7\n",
    "Select 3 or more classification models covered by the curriculum from the sklearn library (Logistic regression, K Nearest Neighbors, Naive Bayes, Decision Tree, Adaboost or Random forest). With the chosen models create an Ensemble model for predicting future probabilities of conflicts using the sklearn VotingClassifier. Report/plot your results via the appropriate metrics for both training and validation set. Do this for all y's (t1, t2, t3, t4, t5). You can use default weighting or justify some other weighting you might find prudent. Feel free to use default hyper parameters or tweak them as you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.8\n",
    "Explain the logic(s) behind Ensemble models and argue for why they might be preferable compared to individual models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.9\n",
    "\n",
    "Now, use an Xgboost classifier to predict future probabilities of conflicts. Report/plot your results via the appropriate metrics for both training and validation set. Do this for all y's (t1, t2, t3, t4, t5). Use some kind of hyper parameter search to optimize the algorithm. You can choose which hyper parameters to optimize and you can choose how to do it. That is using the sklearn random-search, sklearn grid-search, a more manual approach like in the some_plotsV2.ipynb notebook or a combination of it all (It does not have to be super comprehensive).\n",
    "\n",
    "Explain the intuition behind at least one of the hyper parameters you have chosen to optimize, and justify why you chose this/these hyper parameters for optimization (preferable something here should relate to the concept of Regularization).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.10\n",
    "Plot the A single tree from Xgboost model from 2.9. Then, plot the Feature importance given the Xgboost model from 2.9. Comment on what types of features appears to be most important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.11 \n",
    "Using some Feature Selection scheme of your choice, choose a subset of features (n<N) that does not (or almost not) lower performance. What are the benifits of using such a reduced subset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.12\n",
    "Lastly, create the best model you can (given the curriculum and the data from exam part 1) for predicting future probabilities of conflicts. You are pretty free here, but there are some requsitions: It has to be an ensample. It kan be of different models; of the same model with varying hyper parameters (see Some_plotsV2) of a mixture of both. How much grid searching you do and whether you use a subset of features or not, is up to you.\n",
    "\n",
    "You should report/plot the uncertainty revieled by this ensamble and comment on some substantial interpreation of this uncertainty. Report/plot results, via the appropiate metrics, for both training and validation set and for all y's (t1, t2, t3, t4, t5). Beyond the reporting/plotting of the conventional metrics also produce a \"confusion\" plot (denoting TPs, FPs,TNs and FNs as shown in Some_plotsV2.ipynb). Remeber to choose and justify an appropiate threshold here.\n",
    "\n",
    "While I do expect you to create a model that is doing *fine* compared to the baselines, the most important thing here is thorough and honest evaluation of the model's performance. Also creativity is appriciated so do yourself a favor and have some fun - you are almost done :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.12\n",
    "\n",
    "Load your `y_train_ttime.pkl`, `X_train_ttime.pkl`, `y_test.pkl` and `X_test.pkl`. Now, train your model from 2.12 using `y_train_ttime.pkl` and `X_train_ttime.pkl`. Evaluate the model using the `y_test.pkl` and `X_test.pkl`. How well do you model do? Explain why we saved this test set for last?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
